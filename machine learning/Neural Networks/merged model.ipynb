{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7a8863-d2bc-40b5-85a9-1bde57ab8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, concatenate, Input\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM\n",
    "model1=load_model('dmodel1.keras')\n",
    "model2=load_model('dmodel2.keras')\n",
    "model3=load_model('dmodel3.keras')\n",
    "model4=load_model('dmodel4.keras')\n",
    "model5=load_model('dmodel5.keras')\n",
    "model6=load_model('dmodel6.keras')\n",
    "model7=load_model('dmodel7.keras')\n",
    "model8=load_model('dmodel8.keras')\n",
    "model9=load_model('dmodel9.keras')\n",
    "model10=load_model('dmodel10.keras')\n",
    "model11=load_model('dmodel11.keras')\n",
    "model12=load_model('dmodel12.keras')\n",
    "model13=load_model('dmodel13.keras')\n",
    "model14=load_model('dmodel14.keras')\n",
    "model15=load_model('dmodel15.keras')\n",
    "model16=load_model('dmodel16.keras')\n",
    "model17=load_model('dmodel17.keras')\n",
    "model18=load_model('dmodel18.keras')\n",
    "model19=load_model('dmodel19.keras')\n",
    "model20=load_model('dmodel20.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6845c9c-b984-4fc5-9d7f-eacce55c0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xtest=pd.read_csv(r\"B:/project/test/xtest_np20.csv\")\n",
    "ytest=pd.read_csv(r\"B:/project/ytest/ytest20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301bc4c9-2b2a-4112-81ac-7a1f33f7c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 33s 170ms/step\n",
      "192/192 [==============================] - 31s 160ms/step\n",
      "192/192 [==============================] - 8s 39ms/step\n",
      "192/192 [==============================] - 36s 188ms/step\n",
      "192/192 [==============================] - 29s 149ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 4s 19ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 4s 20ms/step\n",
      "192/192 [==============================] - 3s 15ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n",
      "192/192 [==============================] - 3s 16ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23068\\3805760834.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Make final prediction based on combined predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Flatten final_prediction and ytest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mfinal_prediction_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mytest_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Evaluate final prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_prediction_flat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mytest_flat\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming 'y_test' is the true labels of your test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load all the individual models\n",
    "models = []\n",
    "for i in range(1, 21):  # Assuming your models are named 'model1.h5', 'model2.h5', ..., 'model20.h5'\n",
    "    model = load_model(f'dmodel{i}.keras')\n",
    "    models.append(model)\n",
    "\n",
    "# Assuming 'x_test' is your test data\n",
    "# Make predictions using each individual model\n",
    "predictions = []\n",
    "for model in models:\n",
    "    prediction = model.predict(xtest)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Combine predictions using averaging\n",
    "combined_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "# Make final prediction based on combined predictions\n",
    "# Flatten final_prediction and ytest\n",
    "final_prediction_flat = final_prediction.flatten()\n",
    "ytest_flat = ytest.flatten()\n",
    "\n",
    "# Evaluate final prediction\n",
    "accuracy = np.mean(final_prediction_flat == ytest_flat) \n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50267da2-b0a6-4967-bdf7-a91ccc8de07a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The name \"sequential\" is used 3 times in the model. All layer names should be unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m final_output \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(combined_outputs)  \u001b[38;5;66;03m# Adjust activation based on your task\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create the combined model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m combined_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39mfinal_output)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compile the combined model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\functional.py:166\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    158\u001b[0m         [\n\u001b[0;32m    159\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    160\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    161\u001b[0m         ]\n\u001b[0;32m    162\u001b[0m     ):\n\u001b[0;32m    163\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    164\u001b[0m             inputs, outputs\n\u001b[0;32m    165\u001b[0m         )\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_graph_network(inputs, outputs)\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\functional.py:265\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m _map_graph_network(\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\functional.py:1160\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m all_names:\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_names\u001b[38;5;241m.\u001b[39mcount(name) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1161\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is used \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_names\u001b[38;5;241m.\u001b[39mcount(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes in the model. All layer names should be unique.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1163\u001b[0m         )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m network_nodes, nodes_by_depth, layers, layers_by_depth\n",
      "\u001b[1;31mValueError\u001b[0m: The name \"sequential\" is used 3 times in the model. All layer names should be unique."
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate,Dense,Dropout\n",
    "\n",
    "# Define the input shape for the combined model\n",
    "input_shape = (51000,)  # Adjust input_size based on your input data shape\n",
    "\n",
    "# Define input layer\n",
    "input_shape=4096\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Create a list to hold the outputs of individual models\n",
    "model_outputs = []\n",
    "\n",
    "# Add each individual model as a layer in the new model\n",
    "for model in models:\n",
    "    model_output = model(input_layer)  # Pass input to each individual model\n",
    "    model_outputs.append(model_output)\n",
    "\n",
    "# Combine the outputs of all individual models using Concatenate layer\n",
    "combined_outputs = Concatenate()(model_outputs)\n",
    "\n",
    "# Define the final output layer for the combined model\n",
    "final_output = Dense(1, activation='sigmoid')(combined_outputs)  # Adjust activation based on your task\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=input_layer, outputs=final_output)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the combined model\n",
    "combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b6ae67-9168-41c4-bbc5-abfe84aed9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=Input(shape=(2216,))\n",
    "input2=Input(shape=(1766,))\n",
    "input3=Input(shape=(1496,))\n",
    "input4=Input(shape=(1425,))\n",
    "input5=Input(shape=(1411,))\n",
    "input6=Input(shape=(1553,))\n",
    "input7=Input(shape=(1706,))\n",
    "input8=Input(shape=(1693,))\n",
    "input9=Input(shape=(1718,))\n",
    "input10=Input(shape=(1823,))\n",
    "input11=Input(shape=(1703,))\n",
    "input12=Input(shape=(1718,))\n",
    "input13=Input(shape=(1754,))\n",
    "input14=Input(shape=(1694,))\n",
    "input15=Input(shape=(1774,))\n",
    "input16=Input(shape=(1670,))\n",
    "input17=Input(shape=(1730,))\n",
    "input18=Input(shape=(1646,))\n",
    "input19=Input(shape=(1758,))\n",
    "input20=Input(shape=(1693,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdedfc4-7092-41c7-a469-524f3b30f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = Embedding(input_dim=52000, output_dim=128)(input1)\n",
    "embedding2 = Embedding(input_dim=51000, output_dim=100)(input2)\n",
    "embedding3 = Embedding(input_dim=51000, output_dim=100)(input3)\n",
    "embedding4 = Embedding(input_dim=51000, output_dim=256)(input4)\n",
    "embedding5 = Embedding(input_dim=51000, output_dim=100)(input5)\n",
    "embedding6 = Embedding(input_dim=51000, output_dim=100)(input6)\n",
    "embedding7 = Embedding(input_dim=51000, output_dim=100)(input7)\n",
    "embedding8 = Embedding(input_dim=51000, output_dim=100)(input8)\n",
    "embedding9 = Embedding(input_dim=51000, output_dim=100)(input9)\n",
    "embedding10 = Embedding(input_dim=51000, output_dim=128)(input10)\n",
    "embedding11 = Embedding(input_dim=51000, output_dim=100)(input11)\n",
    "embedding12 = Embedding(input_dim=51000, output_dim=100)(input12)\n",
    "embedding13 = Embedding(input_dim=51000, output_dim=100)(input13)\n",
    "embedding14 = Embedding(input_dim=51000, output_dim=128)(input14)\n",
    "embedding15 = Embedding(input_dim=51000, output_dim=100)(input15)\n",
    "embedding16 = Embedding(input_dim=51000, output_dim=100)(input16)\n",
    "embedding17 = Embedding(input_dim=51000, output_dim=100)(input17)\n",
    "embedding18 = Embedding(input_dim=51000, output_dim=100)(input18)\n",
    "embedding19 = Embedding(input_dim=51000, output_dim=100)(input19)\n",
    "embedding20 = Embedding(input_dim=51000, output_dim=100)(input20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1809bf2-fab3-4c97-9527-d016d63948ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bed073f-890f-4990-b77c-04224da63870",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = LSTM(64)(embedding1)\n",
    "dense1 = Dense(1, activation='sigmoid')(lstm1)\n",
    "lstm2 = LSTM(64)(embedding2)\n",
    "dense2 = Dense(1, activation='sigmoid')(lstm2)\n",
    "lstm4 = LSTM(64)(embedding4)\n",
    "dense4 = Dense(1, activation='sigmoid')(lstm4)\n",
    "lstm5= LSTM(64)(embedding5)\n",
    "dense5 = Dense(1, activation='sigmoid')(lstm5)\n",
    "conv1d3 = Conv1D(128, 5, activation='relu')(embedding3)\n",
    "maxpool3 = GlobalMaxPooling1D()(conv1d3)\n",
    "dense3 = Dense(64, activation='relu')(maxpool3)\n",
    "dropout3 = Dropout(0.5)(dense3)\n",
    "\n",
    "conv1d6 = Conv1D(128, 5, activation='relu')(embedding6)\n",
    "maxpool6 = GlobalMaxPooling1D()(conv1d6)\n",
    "dense6 = Dense(64, activation='relu')(maxpool6)\n",
    "dropout6 = Dropout(0.5)(dense6)\n",
    "\n",
    "conv1d7 = Conv1D(128, 5, activation='relu')(embedding7)\n",
    "maxpool7 = GlobalMaxPooling1D()(conv1d7)\n",
    "dense7 = Dense(64, activation='relu')(maxpool7)\n",
    "dropout7 = Dropout(0.5)(dense7)\n",
    "\n",
    "conv1d8 = Conv1D(128, 5, activation='relu')(embedding8)\n",
    "maxpool8 = GlobalMaxPooling1D()(conv1d8)\n",
    "dense8 = Dense(64, activation='relu')(maxpool8)\n",
    "dropout8 = Dropout(0.5)(dense8)\n",
    "\n",
    "conv1d9 = Conv1D(128, 5, activation='relu')(embedding9)\n",
    "maxpool9 = GlobalMaxPooling1D()(conv1d9)\n",
    "dense9 = Dense(64, activation='relu')(maxpool9)\n",
    "dropout9 = Dropout(0.5)(dense9)\n",
    "\n",
    "conv1d10 = Conv1D(128, 5, activation='relu')(embedding10)\n",
    "maxpool10 = GlobalMaxPooling1D()(conv1d10)\n",
    "dense10 = Dense(64, activation='relu')(maxpool10)\n",
    "dropout10 = Dropout(0.5)(dense10)\n",
    "\n",
    "conv1d11 = Conv1D(128, 5, activation='relu')(embedding11)\n",
    "maxpool11 = GlobalMaxPooling1D()(conv1d11)\n",
    "dense11 = Dense(64, activation='relu')(maxpool11)\n",
    "dropout11 = Dropout(0.5)(dense11)\n",
    "\n",
    "conv1d12 = Conv1D(128, 5, activation='relu')(embedding12)\n",
    "maxpool12 = GlobalMaxPooling1D()(conv1d12)\n",
    "dense12 = Dense(64, activation='relu')(maxpool12)\n",
    "dropout12 = Dropout(0.5)(dense12)\n",
    "\n",
    "conv1d13 = Conv1D(128, 5, activation='relu')(embedding13)\n",
    "maxpool13 = GlobalMaxPooling1D()(conv1d13)\n",
    "dense13 = Dense(64, activation='relu')(maxpool13)\n",
    "dropout13 = Dropout(0.5)(dense13)\n",
    "\n",
    "conv1d14 = Conv1D(128, 5, activation='relu')(embedding14)\n",
    "maxpool14 = GlobalMaxPooling1D()(conv1d14)\n",
    "dense14 = Dense(64, activation='relu')(maxpool14)\n",
    "dropout14 = Dropout(0.5)(dense14)\n",
    "\n",
    "conv1d15 = Conv1D(128, 5, activation='relu')(embedding15)\n",
    "maxpool15 = GlobalMaxPooling1D()(conv1d15)\n",
    "dense15 = Dense(64, activation='relu')(maxpool15)\n",
    "dropout15 = Dropout(0.5)(dense15)\n",
    "\n",
    "conv1d16 = Conv1D(128, 5, activation='relu')(embedding16)\n",
    "maxpool16 = GlobalMaxPooling1D()(conv1d16)\n",
    "dense16 = Dense(64, activation='relu')(maxpool16)\n",
    "dropout16 = Dropout(0.5)(dense16)\n",
    "\n",
    "conv1d17 = Conv1D(128, 5, activation='relu')(embedding17)\n",
    "maxpool17 = GlobalMaxPooling1D()(conv1d17)\n",
    "dense17 = Dense(64, activation='relu')(maxpool17)\n",
    "dropout17 = Dropout(0.5)(dense17)\n",
    "\n",
    "conv1d18 = Conv1D(128, 5, activation='relu')(embedding18)\n",
    "maxpool18 = GlobalMaxPooling1D()(conv1d18)\n",
    "dense18 = Dense(64, activation='relu')(maxpool18)\n",
    "dropout18= Dropout(0.5)(dense18)\n",
    "\n",
    "conv1d19 = Conv1D(128, 5, activation='relu')(embedding19)\n",
    "maxpool19 = GlobalMaxPooling1D()(conv1d19)\n",
    "dense19 = Dense(64, activation='relu')(maxpool19)\n",
    "dropout19 = Dropout(0.5)(dense19)\n",
    "\n",
    "conv20d1 = Conv1D(128, 5, activation='relu')(embedding20)\n",
    "maxpool20 = GlobalMaxPooling1D()(conv20d1)\n",
    "dense20 = Dense(64, activation='relu')(maxpool20)\n",
    "dropout20 = Dropout(0.5)(dense20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac4f7ed7-a6eb-4eca-b7c3-2fb91dd57198",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=concatenate([dense1,dense2,dense4,dense5,dropout3,dense3,dropout6,dense6,dropout7,dense7,dropout8,dense8,dropout9,dense9,dropout10,dense10,dropout11,dense11,dropout12,dense12,dropout13,dense13,dropout14,dense14,dropout15,dense15,dropout16,dense16,dropout17,dense17,dropout18,dense18,dropout19,dense19,dropout20,dense20])\n",
    "shared_dense = Dense(32, activation='relu')(merged)\n",
    "output=Dense(1,activation='sigmoid')(shared_dense)\n",
    "merged_model=Model(inputs=[input1,input2,input3,input4,input5,input6,input7,input8,input9,input10,input11,input12,input13,input14,input15,input16,input17,input18,input19,input20],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3b30ea1-defa-43f4-ae3f-9f414e0e2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged=concatenate([dropout16,dense16,dropout17,dense17,dropout18,dense18,dropout19,dense19])\n",
    "# shared_dense = Dense(32, activation='relu')(merged)\n",
    "# output=Dense(1,activation='sigmoid')(shared_dense)\n",
    "# merged_model=Model(inputs=[input16,input17,input18,input19],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2491fe39-08ad-47fb-88b8-64decbda4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xtrain1=pd.read_csv(r\"B:/project/train/xtrain_np1.csv\",nrows=1000)\n",
    "xtest1=pd.read_csv(r\"B:/project/test/xtest_np1.csv\",nrows=1000)\n",
    "ytest1=pd.read_csv(r\"B:/project/ytest/ytest1.csv\",nrows=1000)\n",
    "ytrain1=pd.read_csv(r\"B:/project/ytrain/ytrain1.csv\",nrows=1000)\n",
    "\n",
    "xtrain2=pd.read_csv(r\"B:/project/train/xtrain_np2.csv\",nrows=1000)\n",
    "xtest2=pd.read_csv(r\"B:/project/test/xtest_np2.csv\",nrows=1000)\n",
    "ytest2=pd.read_csv(r\"B:/project/ytest/ytest2.csv\",nrows=1000)\n",
    "ytrain2=pd.read_csv(r\"B:/project/ytrain/ytrain2.csv\",nrows=1000)\n",
    "\n",
    "xtrain3=pd.read_csv(r\"B:/project/train/xtrain_np3.csv\",nrows=1000)\n",
    "xtest3=pd.read_csv(r\"B:/project/test/xtest_np3.csv\",nrows=1000)\n",
    "ytest3=pd.read_csv(r\"B:/project/ytest/ytest3.csv\",nrows=1000)\n",
    "ytrain3=pd.read_csv(r\"B:/project/ytrain/ytrain3.csv\",nrows=1000)\n",
    "\n",
    "xtrain4=pd.read_csv(r\"B:/project/train/xtrain_np4.csv\",nrows=1000)\n",
    "xtest4=pd.read_csv(r\"B:/project/test/xtest_np4.csv\",nrows=1000)\n",
    "ytest4=pd.read_csv(r\"B:/project/ytest/ytest4.csv\",nrows=1000)\n",
    "ytrain4=pd.read_csv(r\"B:/project/ytrain/ytrain4.csv\",nrows=1000)\n",
    "\n",
    "xtrain5=pd.read_csv(r\"B:/project/train/xtrain_np5.csv\",nrows=1000)\n",
    "xtest5=pd.read_csv(r\"B:/project/test/xtest_np5.csv\",nrows=1000)\n",
    "ytest5=pd.read_csv(r\"B:/project/ytest/ytest5.csv\",nrows=1000)\n",
    "ytrain5=pd.read_csv(r\"B:/project/ytrain/ytrain5.csv\",nrows=1000)\n",
    "\n",
    "xtrain6=pd.read_csv(r\"B:/project/train/xtrain_np6.csv\",nrows=1000)\n",
    "xtest6=pd.read_csv(r\"B:/project/test/xtest_np6.csv\",nrows=1000)\n",
    "ytest6=pd.read_csv(r\"B:/project/ytest/ytest6.csv\",nrows=1000)\n",
    "ytrain6=pd.read_csv(r\"B:/project/ytrain/ytrain6.csv\",nrows=1000)\n",
    "\n",
    "xtrain7=pd.read_csv(r\"B:/project/train/xtrain_np7.csv\",nrows=1000)\n",
    "xtest7=pd.read_csv(r\"B:/project/test/xtest_np7.csv\",nrows=1000)\n",
    "ytest7=pd.read_csv(r\"B:/project/ytest/ytest7.csv\",nrows=1000)\n",
    "ytrain7=pd.read_csv(r\"B:/project/ytrain/ytrain7.csv\",nrows=1000)\n",
    "\n",
    "xtrain8=pd.read_csv(r\"B:/project/train/xtrain_np8.csv\",nrows=1000)\n",
    "xtest8=pd.read_csv(r\"B:/project/test/xtest_np8.csv\",nrows=1000)\n",
    "ytest8=pd.read_csv(r\"B:/project/ytest/ytest8.csv\",nrows=1000)\n",
    "ytrain8=pd.read_csv(r\"B:/project/ytrain/ytrain8.csv\",nrows=1000)\n",
    "\n",
    "xtrain9=pd.read_csv(r\"B:/project/train/xtrain_np9.csv\",nrows=1000)\n",
    "xtest9=pd.read_csv(r\"B:/project/test/xtest_np9.csv\",nrows=1000)\n",
    "ytest9=pd.read_csv(r\"B:/project/ytest/ytest9.csv\",nrows=1000)\n",
    "ytrain9=pd.read_csv(r\"B:/project/ytrain/ytrain9.csv\",nrows=1000)\n",
    "\n",
    "xtrain10=pd.read_csv(r\"B:/project/train/xtrain_np10.csv\",nrows=1000)\n",
    "xtest10=pd.read_csv(r\"B:/project/test/xtest_np10.csv\",nrows=1000)\n",
    "ytest10=pd.read_csv(r\"B:/project/ytest/ytest10.csv\",nrows=1000)\n",
    "ytrain10=pd.read_csv(r\"B:/project/ytrain/ytrain10.csv\",nrows=1000)\n",
    "\n",
    "xtrain11=pd.read_csv(r\"B:/project/train/xtrain_np11.csv\",nrows=1000)\n",
    "xtest11=pd.read_csv(r\"B:/project/test/xtest_np11.csv\",nrows=1000)\n",
    "ytest11=pd.read_csv(r\"B:/project/ytest/ytest11.csv\",nrows=1000)\n",
    "ytrain11=pd.read_csv(r\"B:/project/ytrain/ytrain11.csv\",nrows=1000)\n",
    "\n",
    "xtrain12=pd.read_csv(r\"B:/project/train/xtrain_np12.csv\",nrows=1000)\n",
    "xtest12=pd.read_csv(r\"B:/project/test/xtest_np12.csv\",nrows=1000)\n",
    "ytest12=pd.read_csv(r\"B:/project/ytest/ytest12.csv\",nrows=1000)\n",
    "ytrain12=pd.read_csv(r\"B:/project/ytrain/ytrain12.csv\",nrows=1000)\n",
    "\n",
    "xtrain13=pd.read_csv(r\"B:/project/train/xtrain_np13.csv\",nrows=1000)\n",
    "xtest13=pd.read_csv(r\"B:/project/test/xtest_np13.csv\",nrows=1000)\n",
    "ytest13=pd.read_csv(r\"B:/project/ytest/ytest13.csv\",nrows=1000)\n",
    "ytrain13=pd.read_csv(r\"B:/project/ytrain/ytrain13.csv\",nrows=1000)\n",
    "\n",
    "xtrain14=pd.read_csv(r\"B:/project/train/xtrain_np14.csv\",nrows=1000)\n",
    "xtest14=pd.read_csv(r\"B:/project/test/xtest_np14.csv\",nrows=1000)\n",
    "ytest14=pd.read_csv(r\"B:/project/ytest/ytest14.csv\",nrows=1000)\n",
    "ytrain14=pd.read_csv(r\"B:/project/ytrain/ytrain14.csv\",nrows=1000)\n",
    "\n",
    "xtrain15=pd.read_csv(r\"B:/project/train/xtrain_np15.csv\",nrows=1000)\n",
    "xtest15=pd.read_csv(r\"B:/project/test/xtest_np15.csv\",nrows=1000)\n",
    "ytest15=pd.read_csv(r\"B:/project/ytest/ytest15.csv\",nrows=1000)\n",
    "ytrain15=pd.read_csv(r\"B:/project/ytrain/ytrain15.csv\",nrows=1000)\n",
    "\n",
    "xtrain16=pd.read_csv(r\"B:/project/train/xtrain_np16.csv\",nrows=1000)\n",
    "xtest16=pd.read_csv(r\"B:/project/test/xtest_np16.csv\",nrows=1000)\n",
    "ytest16=pd.read_csv(r\"B:/project/ytest/ytest16.csv\",nrows=1000)\n",
    "ytrain16=pd.read_csv(r\"B:/project/ytrain/ytrain16.csv\",nrows=1000)\n",
    "\n",
    "xtrain17=pd.read_csv(r\"B:/project/train/xtrain_np17.csv\",nrows=1000)\n",
    "xtest17=pd.read_csv(r\"B:/project/test/xtest_np17.csv\",nrows=1000)\n",
    "ytest17=pd.read_csv(r\"B:/project/ytest/ytest17.csv\",nrows=1000)\n",
    "ytrain17=pd.read_csv(r\"B:/project/ytrain/ytrain17.csv\",nrows=1000)\n",
    "\n",
    "xtrain18=pd.read_csv(r\"B:/project/train/xtrain_np18.csv\",nrows=1000)\n",
    "xtest18=pd.read_csv(r\"B:/project/test/xtest_np18.csv\",nrows=1000)\n",
    "ytest18=pd.read_csv(r\"B:/project/ytest/ytest18.csv\",nrows=1000)\n",
    "ytrain18=pd.read_csv(r\"B:/project/ytrain/ytrain18.csv\",nrows=1000)\n",
    "\n",
    "xtrain19=pd.read_csv(r\"B:/project/train/xtrain_np19.csv\",nrows=1000)\n",
    "xtest19=pd.read_csv(r\"B:/project/test/xtest_np19.csv\",nrows=1000)\n",
    "ytest19=pd.read_csv(r\"B:/project/ytest/ytest19.csv\",nrows=1000)\n",
    "ytrain19=pd.read_csv(r\"B:/project/ytrain/ytrain19.csv\",nrows=1000)\n",
    "\n",
    "xtrain20=pd.read_csv(r\"B:/project/train/xtrain_np20.csv\",nrows=1000)\n",
    "xtest20=pd.read_csv(r\"B:/project/test/xtest_np20.csv\",nrows=1000)\n",
    "ytest20=pd.read_csv(r\"B:/project/ytest/ytest20.csv\",nrows=1000)\n",
    "ytrain20=pd.read_csv(r\"B:/project/ytrain/ytrain20.csv\",nrows=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be124402-11a0-44bd-94ba-b09cab1e6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain16)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain16 = np.pad(ytrain16, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain16 = ytrain16[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28749284-a525-4e91-a80f-206127c08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Unnamed' with the actual column name you want to drop\n",
    "\n",
    "# Drop rows based on 'Unnamed' column for ytrain DataFrames\n",
    "ytrain1 = ytrain1.drop(ytrain1.columns[ytrain1.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain2 = ytrain2.drop(ytrain2.columns[ytrain2.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain3 = ytrain3.drop(ytrain3.columns[ytrain3.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain4 = ytrain4.drop(ytrain4.columns[ytrain4.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain5 = ytrain5.drop(ytrain5.columns[ytrain5.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain6 = ytrain6.drop(ytrain6.columns[ytrain6.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain7 = ytrain7.drop(ytrain7.columns[ytrain7.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain8 = ytrain8.drop(ytrain8.columns[ytrain8.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain9 = ytrain9.drop(ytrain9.columns[ytrain9.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain10 = ytrain10.drop(ytrain10.columns[ytrain10.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain11 = ytrain11.drop(ytrain11.columns[ytrain11.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain12 = ytrain12.drop(ytrain12.columns[ytrain12.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain13 = ytrain13.drop(ytrain13.columns[ytrain13.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain14 = ytrain14.drop(ytrain14.columns[ytrain14.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain15 = ytrain15.drop(ytrain15.columns[ytrain15.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain16 = ytrain16.drop(ytrain16.columns[ytrain16.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain17 = ytrain17.drop(ytrain17.columns[ytrain17.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain18 = ytrain18.drop(ytrain18.columns[ytrain18.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain19 = ytrain19.drop(ytrain19.columns[ytrain19.columns.str.contains('Unnamed')], axis=1)\n",
    "ytrain20 = ytrain20.drop(ytrain20.columns[ytrain20.columns.str.contains('Unnamed')], axis=1)\n",
    "\n",
    "# Drop rows based on 'Unnamed' column for ytest DataFrames\n",
    "ytest1 = ytest1.drop(ytest1.columns[ytest1.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest2 = ytest2.drop(ytest2.columns[ytest2.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest3 = ytest3.drop(ytest3.columns[ytest3.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest4 = ytest4.drop(ytest4.columns[ytest4.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest5 = ytest5.drop(ytest5.columns[ytest5.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest6 = ytest6.drop(ytest6.columns[ytest6.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest7 = ytest7.drop(ytest7.columns[ytest7.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest8 = ytest8.drop(ytest8.columns[ytest8.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest9 = ytest9.drop(ytest9.columns[ytest9.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest10 = ytest10.drop(ytest10.columns[ytest10.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest11 = ytest11.drop(ytest11.columns[ytest11.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest12 = ytest12.drop(ytest12.columns[ytest12.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest13 = ytest13.drop(ytest13.columns[ytest13.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest14 = ytest14.drop(ytest14.columns[ytest14.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest15 = ytest15.drop(ytest15.columns[ytest15.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest16 = ytest16.drop(ytest16.columns[ytest16.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest17 = ytest17.drop(ytest17.columns[ytest17.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest18 = ytest18.drop(ytest18.columns[ytest18.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest19 = ytest19.drop(ytest19.columns[ytest19.columns.str.contains('Unnamed')], axis=1)\n",
    "ytest20 = ytest20.drop(ytest20.columns[ytest20.columns.str.contains('Unnamed')], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9d5e33-fd1f-4f33-a7db-5007e1673a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain17)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain17 = np.pad(ytrain17, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain17 = ytrain17[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50dfd452-4590-40d9-be5c-75937d779c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain18)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain18 = np.pad(ytrain18, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain18 = ytrain18[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9be063-6abf-495b-a013-b2e01f286f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain19)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain19 = np.pad(ytrain19, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain19 = ytrain19[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36658a7-30ee-4d25-84e2-a43455db59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain20)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain20 = np.pad(ytrain20, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain20 = ytrain20[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5fec3e8-c922-409a-9f75-289de6739f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming xtrain20 is the last array\n",
    "desired_size = 24463\n",
    "current_size = len(xtrain20)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    xtrain20 = np.pad(xtrain20, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    xtrain20 = xtrain20[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97bbb6f1-a86d-4249-b183-12cd533871d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming xtrain20 is the last array\n",
    "desired_size = 24463\n",
    "current_size = len(xtrain12)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    xtrain12 = np.pad(xtrain12, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    xtrain12 = xtrain12[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e4b44de-35c2-4bd4-9b5b-013d5bc5a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "\n",
    "# Assuming you have ytrain1, ytrain2, ..., ytrain20 as individual arrays\n",
    "y_train_list = [ytrain1, ytrain2, ytrain3, ytrain4, ytrain5, ytrain6, ytrain7, ytrain8, ytrain9, ytrain10,\n",
    "                ytrain11, ytrain12, ytrain13, ytrain14, ytrain15, ytrain16, ytrain17, ytrain18, ytrain19, ytrain20]\n",
    "\n",
    "for i in range(len(y_train_list)):\n",
    "    current_size = len(y_train_list[i])\n",
    "\n",
    "    if current_size < desired_size:\n",
    "        # Pad the array to match the desired size\n",
    "        padding = desired_size - current_size\n",
    "        y_train_list[i] = np.pad(y_train_list[i], ((0, padding),), 'constant')\n",
    "    elif current_size > desired_size:\n",
    "        # Truncate the array to match the desired size\n",
    "        y_train_list[i] = y_train_list[i][:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c278f36-0ab8-4fe1-8b2e-4a1c2ce46995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain1)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain1 = np.pad(ytrain1, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain1 = ytrain1[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1abae7e2-17a9-49bb-96f9-3cf06d4ac4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain2)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain2 = np.pad(ytrain2, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain2 = ytrain2[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be37f48c-0437-489d-a142-7be05c1e6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain3)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain3 = np.pad(ytrain3, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain3 = ytrain3[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74aaf4e7-ab94-4afe-85e0-5eb1ab489578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain4)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain4 = np.pad(ytrain4, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain4 = ytrain4[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0363724-373b-4a43-963b-1d03f7237852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain5)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain5 = np.pad(ytrain5, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain5 = ytrain5[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e2ae884-a1d4-4c4a-abbc-0897a2b6f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain6)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain6 = np.pad(ytrain6, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain6 = ytrain6[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "586d7c49-4728-498d-ac86-c7a7fb1d7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain7)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain7 = np.pad(ytrain7, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain7 = ytrain7[:desired_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f57bebf-6dcc-4888-8ec6-c6d56d8c85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain8)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain8 = np.pad(ytrain8, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain8 = ytrain8[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain9)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain9 = np.pad(ytrain9, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain9 = ytrain9[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain10)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain10 = np.pad(ytrain10, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain10 = ytrain10[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain11)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain11= np.pad(ytrain11, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain11= ytrain11[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain12)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain12 = np.pad(ytrain12, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain12 = ytrain12[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain13)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain13 = np.pad(ytrain13, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain13 = ytrain13[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain14)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain14 = np.pad(ytrain14, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain14 = ytrain14[:desired_size]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "desired_size = 24463\n",
    "current_size = len(ytrain15)\n",
    "\n",
    "if current_size < desired_size:\n",
    "    # Pad the array to match the desired size\n",
    "    padding = desired_size - current_size\n",
    "    ytrain15 = np.pad(ytrain15, (0, padding), 'constant')\n",
    "elif current_size > desired_size:\n",
    "    # Truncate the array to match the desired size\n",
    "    ytrain15 = ytrain15[:desired_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b665c048-a68f-4ece-868b-e2e33f0dd35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming xtrain1, xtrain2, ..., xtrain20 and ytrain1, ytrain2, ..., ytrain20 are your data subsets\n",
    "# Replace these with your actual data variables\n",
    "\n",
    "# Combine all subsets into single arrays\n",
    "all_x_train = np.concatenate([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6,xtrain7,xtrain8,xtrain9,xtrain10,xtrain11,xtrain12,xtrain13,xtrain14,xtrain15,xtrain16,xtrain17,xtrain18,xtrain19,xtrain20],axis=1)\n",
    "all_y_train = np.concatenate([ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20],axis=1)\n",
    "\n",
    "# Choose a fraction of the data to use for training (e.g., 10%)\n",
    "subset_fraction = 0.1\n",
    "\n",
    "# Create a subset of the data\n",
    "xtrain_subset, _, ytrain_subset, _ = train_test_split(\n",
    "    all_x_train, all_y_train, train_size=subset_fraction, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cec39e7a-f80f-4c1c-92cf-a6756d727d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list=([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6,xtrain7,xtrain8,xtrain9,xtrain10,xtrain11,xtrain12,xtrain13,xtrain14,xtrain15,xtrain16,xtrain17,xtrain18,xtrain19,xtrain20])\n",
    "y_train_list=([ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20])\n",
    "y_train_list = [ytrain_subset] * len(xtrainlist)\n",
    "subset_size = 1000  # for example\n",
    "\n",
    "# Use a subset of your data for training\n",
    "x_train_subset = [x[:subset_size] for x in x_train_list]\n",
    "y_train_subset = [y[:subset_size] for y in y_train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee318d4d-f5ff-440a-9cbd-4e41d1d7c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list=([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6,xtrain7,xtrain8,xtrain9,xtrain10,xtrain11,xtrain12,xtrain13,xtrain14,xtrain15,xtrain16,xtrain17,xtrain18,xtrain19,xtrain20])\n",
    "ytrainlist=([ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd722a2a-4c97-47a5-8f26-e28032861977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 592, in update_state\n        self.build(y_pred, y_true)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 498, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 646, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 646, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 667, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m merged_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# merged_model.fit([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6,xtrain7,xtrain8,xtrain9,xtrain10,xtrain11,xtrain12,xtrain13,xtrain14,xtrain15,xtrain16,xtrain17,xtrain18,xtrain19,xtrain20],[ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20],epochs=5,batch_size=32)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m merged_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      4\u001b[0m     [xtrain1, xtrain2, xtrain3, xtrain4, xtrain5, xtrain6, xtrain7, xtrain8, xtrain9, xtrain10, xtrain11, xtrain12, xtrain13, xtrain14, xtrain15, xtrain16, xtrain17, xtrain18, xtrain19, xtrain20],\n\u001b[0;32m      5\u001b[0m     [ytrain1, ytrain2, ytrain3, ytrain4, ytrain5, ytrain6, ytrain7, ytrain8, ytrain9, ytrain10, ytrain11, ytrain12, ytrain13, ytrain14, ytrain15, ytrain16, ytrain17, ytrain18, ytrain19, ytrain20],\n\u001b[0;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerylmxmlh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 592, in update_state\n        self.build(y_pred, y_true)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 498, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 646, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 646, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 667, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "merged_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "# merged_model.fit([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6,xtrain7,xtrain8,xtrain9,xtrain10,xtrain11,xtrain12,xtrain13,xtrain14,xtrain15,xtrain16,xtrain17,xtrain18,xtrain19,xtrain20],[ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20],epochs=5,batch_size=32)\n",
    "merged_model.fit(\n",
    "    [xtrain1, xtrain2, xtrain3, xtrain4, xtrain5, xtrain6, xtrain7, xtrain8, xtrain9, xtrain10, xtrain11, xtrain12, xtrain13, xtrain14, xtrain15, xtrain16, xtrain17, xtrain18, xtrain19, xtrain20],\n",
    "    [ytrain1, ytrain2, ytrain3, ytrain4, ytrain5, ytrain6, ytrain7, ytrain8, ytrain9, ytrain10, ytrain11, ytrain12, ytrain13, ytrain14, ytrain15, ytrain16, ytrain17, ytrain18, ytrain19, ytrain20],\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a8e90f6-a526-4d38-9d06-8f700f6a7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ytrain1 Data Type: <class 'numpy.float64'>\n",
      "ytrain2 Data Type: <class 'numpy.float64'>\n",
      "ytrain3 Data Type: <class 'numpy.float64'>\n",
      "ytrain4 Data Type: <class 'numpy.float64'>\n",
      "ytrain5 Data Type: <class 'numpy.float64'>\n",
      "ytrain6 Data Type: <class 'numpy.float64'>\n",
      "ytrain7 Data Type: <class 'numpy.float64'>\n",
      "ytrain8 Data Type: <class 'numpy.float64'>\n",
      "ytrain9 Data Type: <class 'numpy.float64'>\n",
      "ytrain10 Data Type: <class 'numpy.float64'>\n",
      "ytrain11 Data Type: <class 'numpy.float64'>\n",
      "ytrain12 Data Type: <class 'numpy.float64'>\n",
      "ytrain13 Data Type: <class 'numpy.float64'>\n",
      "ytrain14 Data Type: <class 'numpy.float64'>\n",
      "ytrain15 Data Type: <class 'numpy.float64'>\n",
      "ytrain16 Data Type: <class 'numpy.float64'>\n",
      "ytrain17 Data Type: <class 'numpy.float64'>\n",
      "ytrain18 Data Type: <class 'numpy.float64'>\n",
      "ytrain19 Data Type: <class 'numpy.float64'>\n",
      "ytrain20 Data Type: <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "for i, y_train_item in enumerate(ytrainlist, start=1):\n",
    "    print(f\"ytrain{i} Data Type: {type(y_train_item)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4514b24d-c4c7-40d2-b1c4-7e1958a5b076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_metrics at 0x000001E6DD3DD440>\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(y_true, y_pred, sample_weight=None):\n",
    "    y_true_flat = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_flat = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    return super().compute_metrics(y_true_flat, y_pred_flat, sample_weight)\n",
    "print(compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92f67bdf-8b7a-41e2-a0a3-38600f6591c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 617, in call\n        raise NotImplementedError(\n\n    NotImplementedError: Exception encountered when calling layer 'custom_model_6' (type CustomModel).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'custom_model_6' (type CustomModel):\n      • inputs=('tf.Tensor(shape=(None, 2216), dtype=float32)', 'tf.Tensor(shape=(None, 1766), dtype=float32)', 'tf.Tensor(shape=(None, 1496), dtype=float32)', 'tf.Tensor(shape=(None, 1425), dtype=float32)', 'tf.Tensor(shape=(None, 1411), dtype=float32)', 'tf.Tensor(shape=(None, 1553), dtype=float32)', 'tf.Tensor(shape=(None, 1706), dtype=float32)', 'tf.Tensor(shape=(None, 1693), dtype=float32)', 'tf.Tensor(shape=(None, 1718), dtype=float32)', 'tf.Tensor(shape=(None, 1823), dtype=float32)', 'tf.Tensor(shape=(None, 1703), dtype=float32)', 'tf.Tensor(shape=(None, 1718), dtype=float32)', 'tf.Tensor(shape=(None, 1754), dtype=float32)', 'tf.Tensor(shape=(None, 1694), dtype=float32)', 'tf.Tensor(shape=(None, 1774), dtype=float32)', 'tf.Tensor(shape=(None, 1670), dtype=float32)', 'tf.Tensor(shape=(None, 1730), dtype=float32)', 'tf.Tensor(shape=(None, 1646), dtype=float32)', 'tf.Tensor(shape=(None, 1758), dtype=float32)', 'tf.Tensor(shape=(None, 1693), dtype=float32)')\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m merged_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[merged_model\u001b[38;5;241m.\u001b[39mcompute_metrics])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m merged_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     19\u001b[0m     [xtrain1, xtrain2, xtrain3, xtrain4, xtrain5, xtrain6, xtrain7, xtrain8, xtrain9, xtrain10, xtrain11, xtrain12, xtrain13, xtrain14, xtrain15, xtrain16, xtrain17, xtrain18, xtrain19, xtrain20],\n\u001b[0;32m     20\u001b[0m     [ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20],\n\u001b[0;32m     21\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerylmxmlh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 617, in call\n        raise NotImplementedError(\n\n    NotImplementedError: Exception encountered when calling layer 'custom_model_6' (type CustomModel).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'custom_model_6' (type CustomModel):\n      • inputs=('tf.Tensor(shape=(None, 2216), dtype=float32)', 'tf.Tensor(shape=(None, 1766), dtype=float32)', 'tf.Tensor(shape=(None, 1496), dtype=float32)', 'tf.Tensor(shape=(None, 1425), dtype=float32)', 'tf.Tensor(shape=(None, 1411), dtype=float32)', 'tf.Tensor(shape=(None, 1553), dtype=float32)', 'tf.Tensor(shape=(None, 1706), dtype=float32)', 'tf.Tensor(shape=(None, 1693), dtype=float32)', 'tf.Tensor(shape=(None, 1718), dtype=float32)', 'tf.Tensor(shape=(None, 1823), dtype=float32)', 'tf.Tensor(shape=(None, 1703), dtype=float32)', 'tf.Tensor(shape=(None, 1718), dtype=float32)', 'tf.Tensor(shape=(None, 1754), dtype=float32)', 'tf.Tensor(shape=(None, 1694), dtype=float32)', 'tf.Tensor(shape=(None, 1774), dtype=float32)', 'tf.Tensor(shape=(None, 1670), dtype=float32)', 'tf.Tensor(shape=(None, 1730), dtype=float32)', 'tf.Tensor(shape=(None, 1646), dtype=float32)', 'tf.Tensor(shape=(None, 1758), dtype=float32)', 'tf.Tensor(shape=(None, 1693), dtype=float32)')\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_metrics(self, y_true, y_pred):\n",
    "        # Implement your metric computation logic here\n",
    "        accuracy = keras.metrics.BinaryAccuracy()(y_true, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "# Instantiate your custom model\n",
    "merged_model = CustomModel()\n",
    "\n",
    "# Compile the model\n",
    "merged_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[merged_model.compute_metrics])\n",
    "\n",
    "# Train the model\n",
    "merged_model.fit(\n",
    "    [xtrain1, xtrain2, xtrain3, xtrain4, xtrain5, xtrain6, xtrain7, xtrain8, xtrain9, xtrain10, xtrain11, xtrain12, xtrain13, xtrain14, xtrain15, xtrain16, xtrain17, xtrain18, xtrain19, xtrain20],\n",
    "    [ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6,ytrain7,ytrain8,ytrain9,ytrain10,ytrain11,ytrain12,ytrain13,ytrain14,ytrain15,ytrain16,ytrain17,ytrain18,ytrain19,ytrain20],\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0871a-d931-4192-9d76-dd0e37fe6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_shape = (None, 1718)\n",
    "\n",
    "# Assuming xtrain16 is a Pandas DataFrame\n",
    "current_shape = xtrain20.shape\n",
    "\n",
    "if current_shape[1] < desired_shape[1]:\n",
    "    # Pad the features to match the desired shape\n",
    "    padding = desired_shape[1] - current_shape[1]\n",
    "    xtrain20 = xtrain20.pad(width=padding, side='right', fill_value=0)  # Pad with zeros\n",
    "elif current_shape[1] > desired_shape[1]:\n",
    "    # Truncate the features to match the desired shape\n",
    "    xtrain20 = xtrain20.iloc[:, :desired_shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd38ac56-6130-4ad9-95c7-7f1f621da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_shape = (None, 1774)\n",
    "\n",
    "# Assuming xtrain12 is a NumPy array\n",
    "current_shape = xtrain15.shape\n",
    "\n",
    "if current_shape[1] < desired_shape[1]:\n",
    "    # Pad the features to match the desired shape\n",
    "    padding = desired_shape[1] - current_shape[1]\n",
    "    xtrain15 = np.pad(xtrain15, ((0, 0), (0, padding)), 'constant')\n",
    "elif current_shape[1] > desired_shape[1]:\n",
    "    # Truncate the features to match the desired shape\n",
    "    xtrain15 = xtrain15[:, :desired_shape[1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3482e050-322d-48f6-9c9d-3620e0ca4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_shape = (None, 1774)\n",
    "\n",
    "# Assuming xtrain12 is a NumPy array\n",
    "current_shape = xtrain15.shape\n",
    "\n",
    "if current_shape[1] < desired_shape[1]:\n",
    "    # Pad the features to match the desired shape\n",
    "    padding = desired_shape[1] - current_shape[1]\n",
    "    xtrain15 = np.pad(xtrain15, ((0, 0), (0, padding)), 'constant')\n",
    "elif current_shape[1] > desired_shape[1]:\n",
    "    # Truncate the features to match the desired shape\n",
    "    xtrain15 = xtrain15[:, :desired_shape[1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ab6029-4668-42de-a064-89caf0a6ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_shape = (None, 1670)\n",
    "\n",
    "# Assuming xtrain16 is a Pandas DataFrame\n",
    "current_shape = xtrain16.shape\n",
    "\n",
    "if current_shape[1] < desired_shape[1]:\n",
    "    # Pad the features to match the desired shape\n",
    "    padding = desired_shape[1] - current_shape[1]\n",
    "    xtrain16 = xtrain16.pad(width=padding, side='right', fill_value=0)  # Pad with zeros\n",
    "elif current_shape[1] > desired_shape[1]:\n",
    "    # Truncate the features to match the desired shape\n",
    "    xtrain16 = xtrain16.iloc[:, :desired_shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9294d6e9-e7c8-42fc-843d-03a86535cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "desired_shape = (None, 1718)\n",
    "\n",
    "# Assuming xtrain12 is a NumPy array\n",
    "current_shape = xtrain12.shape\n",
    "\n",
    "if current_shape[1] < desired_shape[1]:\n",
    "    # Pad the features to match the desired shape\n",
    "    padding = desired_shape[1] - current_shape[1]\n",
    "    xtrain12 = np.pad(xtrain12, ((0, 0), (0, padding)), 'constant')\n",
    "elif current_shape[1] > desired_shape[1]:\n",
    "    # Truncate the features to match the desired shape\n",
    "    xtrain12 = xtrain12[:, :desired_shape[1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ed922-4ca5-42da-bc26-617d3ba37029",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.evaluate([xtest1,xtest2,xtest3,xtest4,xtest5,xtest6,xtest7,xtest8,xtest9,xtest10,xtest12,xtest12,xtest13,xtest14,xtest15,xtest16,xtest17,xtest18,xtest19,xtest20],[ytest1,ytest2,ytest3,ytest4,ytest5,ytest6,ytest7,ytest8,ytest9,ytest10,ytest12,ytest12,ytest13,ytest14,ytest15,ytest16,ytest17,ytest18,ytest19,ytest20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46b6b580-f0d8-4ad7-9c44-98bda6e3be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     generated\n",
       "0          0.0\n",
       "1          0.0\n",
       "2          0.0\n",
       "3          1.0\n",
       "4          1.0\n",
       "..         ...\n",
       "995        1.0\n",
       "996        0.0\n",
       "997        1.0\n",
       "998        0.0\n",
       "999        0.0\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

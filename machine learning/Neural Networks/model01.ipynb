{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca34f269-cb39-4f41-a2d7-fb3d4f15a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import LongformerTokenizer\n",
    "tokenizer=LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# essays=\"Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.In like matter of this, article,In German Suburb, Life Goes On Without Cars,by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, Paris bans driving due to smog, by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.Likewise, in the article,Carfree day is spinning into a big hit in Bogota,by Andrew Selsky says, how programs thats set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isnt that far from you and doesnt need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\"\n",
    "\n",
    "df=pd.read_csv(r\"B:\\project\\chunks\\chunk_1.csv\")\n",
    "essays = df['text'].tolist()\n",
    "tokens = tokenizer(essays, return_tensors='pt', padding=True, truncation=True)\n",
    "x=tokens['input_ids']\n",
    "y=df['generated'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edabb2a-f226-4d3b-ad76-4d107c2da3c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming xtrain is a PyTorch tensor\n",
    "xtrain_np = xtrain.numpy()\n",
    "xtest_np=xtest.numpy()\n",
    "# x_np=x.numpy()\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=100000, output_dim=128, input_length=2334))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ytrain = df['generated'].values\n",
    "model.fit(xtrain_np, ytrain, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4b56a05c-f0e8-4456-aea8-8d562d8e4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "381/381 [==============================] - 551s 1s/step - loss: 0.3044 - accuracy: 0.9130\n",
      "Epoch 2/5\n",
      "381/381 [==============================] - 726s 2s/step - loss: 0.2922 - accuracy: 0.9149\n",
      "Epoch 3/5\n",
      "381/381 [==============================] - 1036s 3s/step - loss: 0.2919 - accuracy: 0.9149\n",
      "Epoch 4/5\n",
      "381/381 [==============================] - 1052s 3s/step - loss: 0.2916 - accuracy: 0.9149\n",
      "Epoch 5/5\n",
      "381/381 [==============================] - 1014s 3s/step - loss: 0.2919 - accuracy: 0.9149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x227ac49d810>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming xtrain is a PyTorch tensor\n",
    "xtrain_np = xtrain.numpy()\n",
    "\n",
    "input_length<=2335\n",
    "\n",
    "# Pad or truncate the input sequences to match the determined max length\n",
    "xtrain_np_padded = pad_sequences(xtrain_np, maxlen=input_length, padding='post', truncating='post')\n",
    "\n",
    "# Now, input_length will be dynamically determined based on your training data\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=100000, output_dim=128, input_length=input_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ytrain = df['generated'].values\n",
    "model.fit(xtrain_np_padded, ytrain, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "aa84ff5b-c47c-4448-9f1f-5f6b1b1c6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model01.keras\")\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a5031cb6-10d4-4e62-8201-bcfef2038fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 870ms/step\n",
      "0: It is Human-generated text\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "ml = load_model(\"model01.keras\")\n",
    "xnew = \"hi my name is yogeskumaran i am from madurai, where are you from\"\n",
    "\n",
    "# Tokenize the input text\n",
    "xnew_tokens = tokenizer.encode(xnew,truncation=True,padding=True,return_tensors=\"np\")\n",
    "\n",
    "max_sequence_length <= 2335\n",
    "xnew_padded = pad_sequences(xnew_tokens, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Use the padded tensor for prediction\n",
    "prediction = ml.predict(xnew_padded)\n",
    "\n",
    "# Set a threshold for classification\n",
    "threshold = 0.5\n",
    "prediction_value = prediction[0, 0]\n",
    "\n",
    "# Check the prediction and print the result\n",
    "if prediction_value >= threshold:\n",
    "    print(\"1: It is an AI-generated text\")\n",
    "else:\n",
    "    print(\"0: It is Human-generated text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8b19f996-f49d-40ef-bc3c-31819f2427d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 867ms/step\n",
      "[[0.0804094]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "ml = load_model(\"model01.keras\")\n",
    "xnew=\"Studying Science and History at Generic_School offers two very different experiences each with its own advantages Science is a constantly evolving field with new advances and techniques being discovered daily The school provides expert guidance and modern equipment making it an ideal place to explore the complex theories and technologies of the subject In contrast History offers an opportunity to delve into the past and investigate the origins of our civilization and culture Through lectures and field trips Generic_School s expert faculty provides students with a solid foundation of knowledge in the subject while also allowing them to study and discuss unique interpretations of traditional narratives Both Science and History have their advantages but in the end it comes down to each individual student s interests and goals If you are passionate about understanding the complexities of our world then studying Science is the perfect choice If on the other hand you enjoy delving into the past and uncovering the stories of our ancestors then History is the subject for you Whichever path you choose to take the experienced faculty and resources at Generic_School will provide the guidance you need to succeed and make the most of your studies\"\n",
    "# Tokenize the input text\n",
    "xnew_tokens = tokenizer.encode(xnew,truncation=True,padding=True,return_tensors=\"np\")\n",
    "\n",
    "max_sequence_length<=2500\n",
    "xnew_padded = pad_sequences(xnew_tokens, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "prediction = ml.predict(xnew_padded)\n",
    "print(prediction)\n",
    "# Set a threshold for classification\n",
    "# threshold = 0.5\n",
    "# prediction_value = prediction[0, 0]\n",
    "\n",
    "# # Check the prediction and print the result\n",
    "# if prediction_value >= threshold:\n",
    "#     print(\"1: It is an AI-generated text\")\n",
    "# else:\n",
    "#     print(\"0: It is Human-generated text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da706293-9c22-4de0-b94b-52c232eec431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 25 variables whereas the saved optimizer has 1 variables. \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 2500) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m xnew_padded \u001b[38;5;241m=\u001b[39m pad_sequences(xnew_tokens, maxlen\u001b[38;5;241m=\u001b[39mmax_sequence_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, truncating\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Use the padded tensor for prediction\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m ml\u001b[38;5;241m.\u001b[39mpredict(xnew_padded)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "File \u001b[1;32mA:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2mf6uqyf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 2500) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import LongformerTokenizer\n",
    "tokenizer=LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "ml=load_model('combinedmodel.keras')\n",
    "xnew=\"Studying Science and History at Generic_School offers two very different experiences each with its own advantages Science is a constantly evolving field with new advances and techniques being discovered daily The school provides expert guidance and modern equipment making it an ideal place to explore the complex theories and technologies of the subject In contrast History offers an opportunity to delve into the past and investigate the origins of our civilization and culture Through lectures and field trips Generic_School s expert faculty provides students with a solid foundation of knowledge in the subject while also allowing them to study and discuss unique interpretations of traditional narratives Both Science and History have their advantages but in the end it comes down to each individual student s interests and goals If you are passionate about understanding the complexities of our world then studying Science is the perfect choice If on the other hand you enjoy delving into the past and uncovering the stories of our ancestors then History is the subject for you Whichever path you choose to take the experienced faculty and resources at Generic_School will provide the guidance you need to succeed and make the most of your studies\"\n",
    "# Tokenize the input text\n",
    "xnew_tokens = tokenizer.encode(xnew,truncation=True,padding=True,return_tensors=\"np\")\n",
    "\n",
    "max_sequence_length=2500\n",
    "xnew_padded = pad_sequences(xnew_tokens, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Use the padded tensor for prediction\n",
    "prediction = ml.predict(xnew_padded)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

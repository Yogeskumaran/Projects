{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b858531-5f89-414e-85b4-fc08caec6f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(28538, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "filepath=[r\"B:\\project\\human\\human_chunk_12.csv\",r\"B:\\project\\ai\\ai_chunk_12.csv\"]\n",
    "dfs = [pd.read_csv(filepath) for filepath in filepath]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9322f5-7f3d-4092-906c-a036ee8926ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google has had Ears that EAN drive independent...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It has been shown in scientific studies that p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car have become a part of everyday life over t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Humans are slowly destroying earth and scienti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Principal I really thank that students sh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28533</th>\n",
       "      <td>I agree that online or video Conferencing dist...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28534</th>\n",
       "      <td>The Benefits of Limiting Car Usage Many cities...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28535</th>\n",
       "      <td>As an eighth grade student I believe that atte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28536</th>\n",
       "      <td>It is a common belief that having a broad know...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28537</th>\n",
       "      <td>I firmly believe that governments worldwide sh...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28538 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "0      Google has had Ears that EAN drive independent...        0.0\n",
       "1      It has been shown in scientific studies that p...        0.0\n",
       "2      Car have become a part of everyday life over t...        0.0\n",
       "3      Humans are slowly destroying earth and scienti...        0.0\n",
       "4      Dear Principal I really thank that students sh...        0.0\n",
       "...                                                  ...        ...\n",
       "28533  I agree that online or video Conferencing dist...        1.0\n",
       "28534  The Benefits of Limiting Car Usage Many cities...        1.0\n",
       "28535  As an eighth grade student I believe that atte...        1.0\n",
       "28536  It is a common belief that having a broad know...        1.0\n",
       "28537  I firmly believe that governments worldwide sh...        1.0\n",
       "\n",
       "[28538 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d344a7-7bc2-4ad2-bb0f-c2dc5b262c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay=df['text'].tolist()\n",
    "from transformers import LongformerTokenizer\n",
    "tokenizer=LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "essay=df['text'].tolist()\n",
    "tokens=tokenizer(essay,return_tensors='pt',padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171e4b21-4725-477a-9466-30d15508bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# input_ids_size = tokens['input_ids'].size()\n",
    "# attention_mask_size = tokens['attention_mask'].size()\n",
    "# print(\"Input IDs size:\", input_ids_size)\n",
    "# print(\"Attention Mask size:\", attention_mask_size)\n",
    "# # Assuming 'tokens' is the result of tokenizer\n",
    "# unique_tokens = set(token.item() for seq in tokens['input_ids'] for token in seq)\n",
    "# vocabulary_size = len(unique_tokens)\n",
    "\n",
    "# print(\"Vocabulary Size:\", vocabulary_size)\n",
    "# # Assuming 'input_ids' is the first element in xtrain\n",
    "# max_sequence_length_x = max(len(seq) for seq in xtrain)\n",
    "\n",
    "# # Print or use the max_sequence_length_x in your pad_sequences and Embedding layer\n",
    "# print(\"Max Sequence Length:\", max_sequence_length_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f191a20-f74e-4437-9b8f-86374bf462f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "90/90 [==============================] - 60s 654ms/step - loss: 0.4010 - accuracy: 0.8272\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 59s 650ms/step - loss: 0.0536 - accuracy: 0.9836\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 66s 736ms/step - loss: 0.0170 - accuracy: 0.9960\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 69s 763ms/step - loss: 0.0066 - accuracy: 0.9991\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 66s 729ms/step - loss: 0.0030 - accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bca5935950>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "xtrain_np=xtrain.numpy()\n",
    "input_length = None\n",
    "input_layer = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=51000, output_dim=100, input_length=input_length)(input_layer)\n",
    "conv1d_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "maxpool_layer = GlobalMaxPooling1D()(conv1d_layer)\n",
    "dense_layer = Dense(64, activation='relu')(maxpool_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming xtrain_np and ytrain are NumPy arrays\n",
    "model.fit(xtrain_np, ytrain, epochs=5, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31c50cd-a9d3-4832-8ca4-633bdb4f9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "72/72 [==============================] - 62s 845ms/step - loss: 0.4778 - accuracy: 0.7849 - val_loss: 0.1319 - val_accuracy: 0.9619\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 60s 839ms/step - loss: 0.0747 - accuracy: 0.9772 - val_loss: 0.0448 - val_accuracy: 0.9866\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 61s 845ms/step - loss: 0.0221 - accuracy: 0.9948 - val_loss: 0.0402 - val_accuracy: 0.9873\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 61s 841ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.0377 - val_accuracy: 0.9888\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 61s 844ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0385 - val_accuracy: 0.9884\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 0.0385 - accuracy: 0.9884\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 60s 837ms/step - loss: 0.0134 - accuracy: 0.9964 - val_loss: 0.0026 - val_accuracy: 0.9998\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 62s 857ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0014 - val_accuracy: 0.9998\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 61s 852ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 0.9996\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 61s 841ms/step - loss: 8.2659e-04 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 0.9998\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 61s 844ms/step - loss: 7.9983e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 0.9996\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 1.4095e-04 - accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 61s 840ms/step - loss: 9.8047e-04 - accuracy: 0.9998 - val_loss: 8.7316e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 61s 844ms/step - loss: 7.6342e-04 - accuracy: 0.9999 - val_loss: 7.5220e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 61s 841ms/step - loss: 3.8096e-04 - accuracy: 1.0000 - val_loss: 5.3259e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 75s 1s/step - loss: 3.2303e-04 - accuracy: 0.9999 - val_loss: 4.5783e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 77s 1s/step - loss: 2.8083e-04 - accuracy: 1.0000 - val_loss: 3.7787e-05 - val_accuracy: 1.0000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 2.8339e-05 - accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 77s 1s/step - loss: 2.2712e-04 - accuracy: 1.0000 - val_loss: 1.8644e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 76s 1s/step - loss: 1.8983e-04 - accuracy: 1.0000 - val_loss: 1.6399e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 350s 5s/step - loss: 1.9229e-04 - accuracy: 1.0000 - val_loss: 1.4650e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 93s 1s/step - loss: 1.4028e-04 - accuracy: 1.0000 - val_loss: 1.3219e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 66s 918ms/step - loss: 1.4432e-04 - accuracy: 1.0000 - val_loss: 1.2185e-05 - val_accuracy: 1.0000\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 9.0943e-06 - accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 67s 929ms/step - loss: 1.2035e-04 - accuracy: 1.0000 - val_loss: 7.1365e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 67s 930ms/step - loss: 1.2042e-04 - accuracy: 1.0000 - val_loss: 7.0001e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 67s 931ms/step - loss: 1.3011e-04 - accuracy: 1.0000 - val_loss: 6.2162e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 67s 926ms/step - loss: 1.0387e-04 - accuracy: 1.0000 - val_loss: 5.6780e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 67s 931ms/step - loss: 8.6275e-05 - accuracy: 1.0000 - val_loss: 5.8165e-06 - val_accuracy: 1.0000\n",
      "143/143 [==============================] - 3s 24ms/step - loss: 4.1889e-06 - accuracy: 1.0000\n",
      "Average accuracy:  0.9976784944534302\n"
     ]
    }
   ],
   "source": [
    "# Define your model\n",
    "input_length = None\n",
    "input_layer = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=51000, output_dim=100, input_length=input_length)(input_layer)\n",
    "conv1d_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "maxpool_layer = GlobalMaxPooling1D()(conv1d_layer)\n",
    "dense_layer = Dense(64, activation='relu')(maxpool_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Perform cross-validation\n",
    "# Perform cross-validation\n",
    "accuracies = []\n",
    "for train_index, test_index in kfold.split(xtrain_np):\n",
    "    xtrain_cv, xval_cv = xtrain_np[train_index], xtrain_np[test_index]\n",
    "    ytrain_cv, yval_cv = ytrain.iloc[train_index], ytrain.iloc[test_index]\n",
    "\n",
    "    # Fit the model on the training data with validation data\n",
    "    model.fit(xtrain_cv, ytrain_cv, epochs=5, batch_size=256, \n",
    "              validation_data=(xval_cv, yval_cv), callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    loss, accuracy = model.evaluate(xtest_cv, ytest_cv)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Print the average accuracy\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(\"Average accuracy: \", average_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ab5146-8b6c-4dc0-b72f-d1fe0ac203aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 3s 18ms/step - loss: 0.0306 - accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.030621709302067757, 0.9905396103858948]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_np=xtest.numpy()\n",
    "model.evaluate(xtest_np,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4b8cda-a57d-406b-b732-dbbc7035002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dmodel12.keras\")\n",
    "model.save_weights('dmodel12_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c9cc1f-fc82-4bd5-bcef-8f4af8b7c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "xtrain_np=xtrain.numpy()\n",
    "xtest_np=xtest.numpy()\n",
    "xtrain_np12=xtrain_np\n",
    "xtest_np12=xtest_np\n",
    "import numpy as np\n",
    "xtrain_np12 = np.array(xtrain_np12)\n",
    "xtest_np12=np.array(xtest_np12)\n",
    "np.savetxt('B:/project/test/xtest_np12.csv', xtest_np12, delimiter=',')\n",
    "np.savetxt('B:/project/train/xtrain_np12.csv', xtrain_np12, delimiter=',')\n",
    "ytest.to_csv(\"B:/project/ytest/ytest12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f07d44-278d-44eb-b9ae-6a61306cb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "ytrain.to_csv(\"B:/project/ytrain/ytrain12.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

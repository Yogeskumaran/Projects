{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af5142c-adc5-451c-aae7-ba101c9ea2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(30580, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "filepath=[r\"B:\\project\\human\\human_chunk_10.csv\",r\"B:\\project\\ai\\ai_chunk_10.csv\"]\n",
    "dfs = [pd.read_csv(filepath) for filepath in filepath]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43fdc22-faa2-4a7a-9d1c-86c02270cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay=df['text'].tolist()\n",
    "from transformers import LongformerTokenizer\n",
    "tokenizer=LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "essay=df['text'].tolist()\n",
    "tokens=tokenizer(essay,return_tensors='pt',padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21efe460-8f87-4179-bfad-a1249f450fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# input_ids_size = tokens['input_ids'].size()\n",
    "# attention_mask_size = tokens['attention_mask'].size()\n",
    "# print(\"Input IDs size:\", input_ids_size)\n",
    "# print(\"Attention Mask size:\", attention_mask_size)\n",
    "# # Assuming 'tokens' is the result of tokenizer\n",
    "# unique_tokens = set(token.item() for seq in tokens['input_ids'] for token in seq)\n",
    "# vocabulary_size = len(unique_tokens)\n",
    "\n",
    "# print(\"Vocabulary Size:\", vocabulary_size)\n",
    "# # Assuming 'input_ids' is the first element in xtrain\n",
    "# max_sequence_length_x = max(len(seq) for seq in xtrain)\n",
    "\n",
    "# # Print or use the max_sequence_length_x in your pad_sequences and Embedding layer\n",
    "# print(\"Max Sequence Length:\", max_sequence_length_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf58a3b-b5c4-4304-aec0-43c76d788d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From A:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "96/96 [==============================] - 84s 860ms/step - loss: 0.3517 - accuracy: 0.8627\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 91s 954ms/step - loss: 0.0422 - accuracy: 0.9881\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 91s 944ms/step - loss: 0.0146 - accuracy: 0.9967\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 93s 966ms/step - loss: 0.0043 - accuracy: 0.9996\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 90s 940ms/step - loss: 0.0019 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19e6659a8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "xtrain_np=xtrain.numpy()\n",
    "input_length = None\n",
    "input_layer = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=51000, output_dim=128, input_length=input_length)(input_layer)\n",
    "conv1d_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "maxpool_layer = GlobalMaxPooling1D()(conv1d_layer)\n",
    "dense_layer = Dense(64, activation='relu')(maxpool_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming xtrain_np and ytrain are NumPy arrays\n",
    "model.fit(xtrain_np, ytrain, epochs=5, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5210b4a6-4b5b-439b-ab9b-cfaa333ceca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 5s 27ms/step - loss: 0.0309 - accuracy: 0.9895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.030878392979502678, 0.9895356297492981]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_np=xtest.numpy()\n",
    "model.evaluate(xtest_np,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1d9c8e-6350-43cd-ab30-6069f5336f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dmodel10.keras\")\n",
    "model.save_weights('dmodel10_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617a2fb8-97fa-4d6e-8e31-bfc3b27b0bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "[[0.00024383]]\n",
      "0: It is Human-generated text\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "ml = load_model(\"dmodel10.keras\")\n",
    "# xnew=\"Studying Science and History at Generic_School offers two very different experiences each with its own advantages Science is a constantly evolving field with new advances and techniques being discovered daily The school provides expert guidance and modern equipment making it an ideal place to explore the complex theories and technologies of the subject In contrast History offers an opportunity to delve into the past and investigate the origins of our civilization and culture Through lectures and field trips Generic_School s expert faculty provides students with a solid foundation of knowledge in the subject while also allowing them to study and discuss unique interpretations of traditional narratives Both Science and History have their advantages but in the end it comes down to each individual student s interests and goals If you are passionate about understanding the complexities of our world then studying Science is the perfect choice If on the other hand you enjoy delving into the past and uncovering the stories of our ancestors then History is the subject for you Whichever path you choose to take the experienced faculty and resources at Generic_School will provide the guidance you need to succeed and make the most of your studies\"\n",
    "# xnew = \"hi my name is yogeskumaran i am from madurai, where are you from\"\n",
    "# Tokenize the input text\n",
    "# xnew=\"Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.In like matter of this, article,In German Suburb, Life Goes On Without Cars,by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, Paris bans driving due to smog, by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.Likewise, in the article,Carfree day is spinning into a big hit in Bogota,by Andrew Selsky says, how programs thats set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 10 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isnt that far from you and doesnt need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\"\n",
    "xnew=\"There are alot reasons to keep our the despised method of choosing the President The First reason is because Certainty of Outcome in the text its states that The reason is that the winning candidate s share of the Electoral College invariably exceeds his share of the popular vote Another example from the text is Obama he recieved 61 10 percent of the electoral vote compared to only 51 3 percent of the popular votes cast for him and Romney because all of the states award electoral votes on a winner take all basis Another reason is that the Electiral College requires a presidential candidate to have trans regional appeal in that case no region has enough electoral votes to elect a president a solid regional favorite such as Romney was in the South has no reason to campaign in those states because of the poor economy and because its not a wealthy state he wont be able to campaign heavily So he left with no votes and would have to find his Electoral College votes in a more suitable area In the text its states that It is a Desriable result because a candidate with regional appeal is unlikely to be a successful president Also the Electoral College restores some of the weight in the political balance the large states by population lose by virtue of the mal apportionment Senate decreed in the Constitution My proof is in the text The popular vote was very close in Florida in 2012 nevertheless Obama who won the that vote got 29 electoral votes A same victory by the same margin in Wynoming would net the winner only 3 electoral votes So other things equal a large state gets more attention compared to a small state And Finally the Electoral College avoids the problemof elections in which no candidate recieves a majority of the votes cast For example Richard Nixon in 68 and Bill Clinton in 92 both had on 43 percent plurality of the popular votes while winning a majority in the in the Electoral College In Conclusion I think that the we should keep Electoral Colleges and changing to election by popular voting to vote for a President\"\n",
    "xnew_tokens = tokenizer.encode(xnew,truncation=True,padding=True,return_tensors=\"np\")\n",
    "\n",
    "max_sequence_length=1823\n",
    "xnew_padded = pad_sequences(xnew_tokens, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "prediction = ml.predict(xnew_padded)\n",
    "print(prediction)\n",
    "# Set a threshold for classification\n",
    "threshold = 0.5\n",
    "prediction_value = prediction[0, 0]\n",
    "\n",
    "# Check the prediction and print the result\n",
    "if prediction_value >= threshold:\n",
    "    print(\"1: It is an AI-generated text\")\n",
    "else:\n",
    "    print(\"0: It is Human-generated text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d75e0e3-123d-488f-8962-33cf601a2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "xtrain_np=xtrain.numpy()\n",
    "xtest_np=xtest.numpy()\n",
    "xtrain_np10=xtrain_np\n",
    "xtest_np10=xtest_np\n",
    "import numpy as np\n",
    "xtrain_np10 = np.array(xtrain_np10)\n",
    "xtest_np10=np.array(xtest_np10)\n",
    "np.savetxt('B:/project/test/xtest_np10.csv', xtest_np10, delimiter=',')\n",
    "np.savetxt('B:/project/train/xtrain_np10.csv', xtrain_np10, delimiter=',')\n",
    "ytest.to_csv(\"B:/project/ytest/ytest10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b122d2-2805-4a22-bc5a-45391b33b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens['input_ids']\n",
    "y=df['generated']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "ytrain.to_csv(\"B:/project/ytrain/ytrain10.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

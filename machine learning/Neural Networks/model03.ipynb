{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "410af3d2-687f-4e0f-a097-83f7f9e5ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import LongformerTokenizer\n",
    "tokenizer=LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "df=pd.read_csv(r\"B:/project/chunks/chunk_3.csv\")\n",
    "essay=df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05f42498-38e4-4c7d-a059-e98719324ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer(essay,return_tensors='pt',padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c493ea8-b150-46c7-b6a1-59497a0b6915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs size: torch.Size([12180, 1334])\n",
      "Attention Mask size: torch.Size([12180, 1334])\n"
     ]
    }
   ],
   "source": [
    "input_ids_size = tokens['input_ids'].size()\n",
    "attention_mask_size = tokens['attention_mask'].size()\n",
    "print(\"Input IDs size:\", input_ids_size)\n",
    "print(\"Attention Mask size:\", attention_mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad0cf14d-ac20-4f37-9ba6-12cbe66aa9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 28449\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokens' is the result of tokenizer\n",
    "unique_tokens = set(token.item() for seq in tokens['input_ids'] for token in seq)\n",
    "vocabulary_size = len(unique_tokens)\n",
    "\n",
    "print(\"Vocabulary Size:\", vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d63dbb0-6a65-41df-9320-3c4cd06a6289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 1334\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'input_ids' is the first element in xtrain\n",
    "max_sequence_length_x = max(len(seq) for seq in xtrain)\n",
    "\n",
    "# Print or use the max_sequence_length_x in your pad_sequences and Embedding layer\n",
    "print(\"Max Sequence Length:\", max_sequence_length_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18dbe881-02c8-40fa-95cc-fdb00f6ce446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b08ba5aa-8150-4076-9b46-e6efdca0187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=tokens['input_ids']\n",
    "y=df['generated'].values\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b4b5eb2-1bba-4e3d-94ec-524998a9b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F460F2E520> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x000001F460F2E520>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F460F2E520> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x000001F460F2E520>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "305/305 [==============================] - 207s 671ms/step - loss: 0.0223 - accuracy: 0.9967\n",
      "Epoch 2/5\n",
      "305/305 [==============================] - 216s 707ms/step - loss: 7.1295e-05 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "305/305 [==============================] - 210s 689ms/step - loss: 3.9321e-05 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "305/305 [==============================] - 218s 715ms/step - loss: 2.4669e-05 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "305/305 [==============================] - 218s 714ms/step - loss: 1.6103e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f455d95d50>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_length=1500\n",
    "xtrain_padded=pad_sequences(xtrain,padding='post',maxlen=input_length)\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=51000,output_dim=100,input_length=input_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(xtrain_padded,ytrain,epochs=5,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8443937-a8cd-4951-9778-f83fd52a1092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 222ms/step\n",
      "[[0.0804094]]\n",
      "0: It is Human-generated text\n"
     ]
    }
   ],
   "source": [
    "# xnew='Attending a four year high school has significant benefits that cannot be overlooked While some students may argue that attending a three year high school is just as beneficial there are several reasons why a four year high school is the better option Firstly attending a four year high school allows students to take more advanced courses Three year high schools may not offer the same level of advanced courses as four year high schools This means that students may not be able to take courses that are necessary for their future careers For example many colleges require students to take advanced math and science courses in high school and three year high schools may not offer these courses Secondly attending a four year high school allows students to participate in more extracurricular activities Many four year high schools offer a wider range of extracurricular activities than three year high schools This means that students can participate in activities that they are passionate about such as sports music or clubs These activities can also help students develop important skills such as teamwork leadership and communication Thirdly attending a four year high school can help students prepare for college Many colleges require students to take standardized tests such as the SAT or ACT in order to be considered for admission Four year high schools typically offer more resources and support to help students prepare for these tests This means that students who attend a four year high school may have a better chance of getting into their desired college Finally attending a four year high school can provide students with a more structured and supportive environment Four year high schools typically have larger student bodies than three year high schools which means that students can meet more people and make more friends Additionally four year high schools often have more resources such as counselors and tutors to help students with their academic and personal needs In conclusion attending a four year high school has significant benefits that cannot be overlooked From advanced courses to extracurricular activities college preparation and a supportive environment a four year high school can provide students with the tools they need to succeed in the future While some may argue that attending a three year high school is just as beneficial the evidence suggests that a four year high school is the better option'\n",
    "xnew_tokens = tokenizer.encode(xnew,truncation=True,padding=True,return_tensors=\"np\")\n",
    "max_sequence_length = 2335\n",
    "xnew_padded = pad_sequences(xnew_tokens,maxlen=max_sequence_length ,padding='post', truncating='post')\n",
    "prediction = ml.predict(xnew_padded)\n",
    "print(prediction)\n",
    "# Set a threshold for classification\n",
    "threshold = 0.5\n",
    "prediction_value = prediction[0, 0]\n",
    "\n",
    "# Check the prediction and print the result\n",
    "if prediction_value >= threshold:\n",
    "    print(\"1: It is an AI-generated text\")\n",
    "else:\n",
    "    print(\"0: It is Human-generated text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea699e5d-8908-456c-a1ef-771e976c1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model03.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284b126-144e-41e3-b437-311f135cced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4580d-096a-415c-aad5-f3f2006d361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_length = 1500\n",
    "\n",
    "xtrain_padded = pad_sequences(xtrain, padding='post', maxlen=input_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=51000, output_dim=100, input_length=1500))\n",
    "model.add(LSTM(64, return_sequences=False))  # Ensure return_sequences is set to False for the last LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain_padded, ytrain, epochs=5, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
